{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 自动求导：自动微分\n",
    "> PyTorch使用了一种叫做自动微分的技术。也就是说，它会有一个记录我们所有执行操作的记录器，之后再回放记录来计算我们的梯度。这一技术在构建神经网络时尤其有效，因为我们可以通过计算前路参数的微分来节省时间。\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "\n",
    "在pytorch中所有神经网络的中心，就是autograd包。让我们先快速的访问一下它，然后训练我们自己的第一个神经网络。\n",
    "\n",
    "autograd包为所有在tensor上的的操作提供自动微分。它是一个运行时刻定义的框架，这意味着你的反向传播(backprop)是在你的代码运行时候定义的，取决于你代码怎么运行，并且每次迭代都是不同的。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## tensor   \n",
    "\n",
    "**torch.tensor是自动求导的中心类。如果你把它的属性.requires_grad设置成true，它将开始追踪所有在它上面的操作。当你完成你所有的计算之后，你可以调用.backward()方法来把所有的梯度自动计算一遍。此tensor的梯度将会累积到它的.grad属性里面。**\n",
    "\n",
    "- 如果你要停止一个tensor的历史追踪，你可以调用.detach()方法来把计算历史进行分离开来，并且阻止后续的计算被追踪。\n",
    "\n",
    "- 如果要停止追踪历史停止使用多余的内存的话，你可以把代码块藏进 with torch.no_grad():里面。这招在评估模型时特别有用，因为模型可能具有可训练的参数，设置了requires_grad=True，但是评估模型的时候并不需要梯度。训练时候需要，评估时候不需要。\n",
    "\n",
    "- 这里有另一个对于自动求导实现的非常重要的类，Function类。\n",
    "\n",
    "**tensor和function这两个类是内部关联的，一起组成了一个有向无环图（acyclic graph），这个图描述了一个完整的计算历史。每个tensor有一个.grad_fn的属性，它引用了创建张量的函数（除了那些用户自己创建的tensor，它们的grad_fn是None）。**\n",
    "\n",
    "如果你想计算导数，你可以在tensor上面调用.backward()方法。如果tensor是一个标量scalar（就是它只包含了一个元素），那么你backward()的时候不需要指定任何参数。但是如果它不止一个元素，你需要指定一个于tensor的形状匹配的gradient参数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个tensor并设置requires_grad=True来记录它经历的计算。\n",
    "x = torch.ones(2,2,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "# 搞个tensor的操作\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "# y作为一个操作的结果被创建，所以它有grad_fn\n",
    "print(y.grad_fn)\n",
    "\n",
    "# 在y上面多搞点操作\n",
    "z = y * y *3\n",
    "out = z.mean()\n",
    "\n",
    "print(z,out)\n",
    "\n",
    "# .requires_grad_(...)这个方法会原地改变一个已存在的tensor的requires_grad的flag。如果没有指定的话，输入的flag默认是False。\n",
    "a = torch.randn(2,2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "梯度\n",
    "让我们开始反向传播吧。因为out仅包含一个标量，那么out.backward()等价于out.backward(torch.tensor(1.))。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out.backward()\n",
    "print(x.grad)  # 打印gradients梯度 d(out)/dx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在输出了一个值全为4.5的矩阵。我们来看看out这个tensor，我们假设是\"o\"，\n",
    "那么o等于求和再平均，幂函数求导。3/2*(x+2)，因此它结果是4.5\n",
    "\n",
    "从数学的角度讲，如果你有一个向量值组成的函数，那么y的梯度是一个雅可比矩阵。这个地方还没搞清楚。\n",
    "\n",
    "一般说来，torch.autograd是一个专门用于计算雅可比向量的产品的引擎。就是，对于任何给定的向量，计算xxx。\n",
    "公式打不出来。\n",
    "\n",
    "vector-Jacobian product，有人翻译成雅可比向量积\n",
    "\n",
    "向量雅可比产品的特性让输入额外的梯度到无标量输出的模型变得非常方便。\n",
    "\n",
    "现在让我们看看一个关于向量雅可比产品的一个例子："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "你也可以通过把代码段折叠进torch.no_grad():，从而停止由.require_grad=True追踪历史中自动求导"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "也可以通过.detach()来得到一个新的Tensor，同样的内容但是不再要求梯度。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}